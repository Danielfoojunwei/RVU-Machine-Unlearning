# models.yaml â€“ Open-weight LLMs and auxiliary models (CPU, GGUF quantized)

llm:
  model_a:
    name: "Qwen2.5-1.5B-Instruct"
    hf_repo: "Qwen/Qwen2.5-1.5B-Instruct-GGUF"
    revision: "main"
    gguf_file: "qwen2.5-1.5b-instruct-q4_k_m.gguf"
    context_length: 4096
    alias: "qwen2.5_1.5b"
    local_dir: "models/llm/qwen2.5-1.5b-instruct"

  model_b:
    name: "Llama-3.2-1B-Instruct"
    hf_repo: "bartowski/Llama-3.2-1B-Instruct-GGUF"
    revision: "main"
    gguf_file: "Llama-3.2-1B-Instruct-Q4_K_M.gguf"
    context_length: 4096
    alias: "llama3.2_1b"
    local_dir: "models/llm/llama-3.2-1b-instruct"

sampling:
  deterministic:
    temperature: 0.0
    top_p: 1.0
    seed: 42
    max_tokens: 1024
    repeat_penalty: 1.0

guard:
  prompt_guard:
    name: "Llama-Prompt-Guard-2-86M"
    hf_repo: "meta-llama/Llama-Prompt-Guard-2-86M"
    revision: "main"
    local_dir: "models/guard/prompt-guard-2-86m"
    threshold: 0.85

embed:
  bge_small:
    name: "BGE-small-en-v1.5"
    hf_repo: "BAAI/bge-small-en-v1.5"
    revision: "main"
    local_dir: "models/embed/bge-small-en-v1.5"
    dimension: 384
